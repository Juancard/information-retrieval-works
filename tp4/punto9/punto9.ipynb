{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import os\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Agrego al path la carpeta modulos\n",
    "sys.path.insert(0, os.path.abspath(\"../../modulos\"))\n",
    "from LexAnalyser import LexAnalyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingresar ruta a words-en.txt: /home/juan/Documentos/UNLu/taller_libre_1/2016/practica/corpus/words-en.txt\n"
     ]
    }
   ],
   "source": [
    "vocabularyPath = raw_input(\"Ingresar ruta a words-en.txt: \")\n",
    "if not(os.path.isfile(vocabularyPath)):\n",
    "    print \"Path a Archivo no es válido\"\n",
    "    exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "la = LexAnalyser({}) #diccionario vacio\n",
    "with codecs.open(vocabularyPath, mode='rt', encoding='utf-8') as f:\n",
    "    termsLen = []\n",
    "    tokensCount = 0\n",
    "    for line in f:\n",
    "        analysed = la.analyse(line)\n",
    "        [termsLen.append(len(t)) for t  in analysed[\"terms\"]]\n",
    "        tokensCount += len(analysed[\"tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bibliografia: Manning: 5.2.1 \"dictionary-as-a-string\n",
    "termsCount = len(termsLen)\n",
    "allCharLen = sum(termsLen)\n",
    "avgTermLen = allCharLen / float(termsCount)\n",
    "ID_TERM_LEN_BYTES = 4\n",
    "POINTER_TO_POSTING_BYTES = 4\n",
    "TERM_POINTER = 3\n",
    "bytesNeeded = termsCount * (ID_TERM_LEN_BYTES + POINTER_TO_POSTING_BYTES + TERM_POINTER + (int(avgTermLen) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens: 354935\n",
      "Cantidad de términos: 354935\n",
      "Cantidad de caracteres totales: 3353624\n",
      "Tamaño promedio de término: 9.45 caracteres\n",
      "Tamaño de Id del término: 4 bytes\n",
      "Tamaño de pointer hacia posting: 4 bytes\n",
      "Tamaño de pointer del término: 3 bytes\n",
      "Tamaño necesario en memoria (como dictionary-as-string): \n",
      "\t354935 * (4 + 4 + 3 + 10) = 7453635 bytes\n",
      "\n",
      "Seran necesarios 7.45 MegaBytes libres en memoria\n"
     ]
    }
   ],
   "source": [
    "print \"Cantidad de tokens: %d\" % tokensCount\n",
    "print u\"Cantidad de términos: %d\" % termsCount\n",
    "print \"Cantidad de caracteres totales: %d\" % allCharLen\n",
    "print u\"Tamaño promedio de término: %.2f caracteres\" % avgTermLen\n",
    "print u\"Tamaño de Id del término: %d bytes\" % ID_TERM_LEN_BYTES\n",
    "print u\"Tamaño de pointer hacia posting: %d bytes\" % POINTER_TO_POSTING_BYTES\n",
    "print u\"Tamaño de pointer del término: %d bytes\" % TERM_POINTER\n",
    "print u\"Tamaño necesario en memoria (como dictionary-as-string): \" \n",
    "print \"\\t%d * (%d + %d + %d + %d) = %d bytes\" % (termsCount, ID_TERM_LEN_BYTES, POINTER_TO_POSTING_BYTES, TERM_POINTER,(int(avgTermLen) + 1), bytesNeeded)\n",
    "\n",
    "print \"\\nSeran necesarios %.2f MegaBytes libres en memoria\" % (bytesNeeded / 1000000.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distribucion de las longitudes de las palabras\n",
    "# NO HACE FALTA, LO HACE EL HISTOGRAMA\n",
    "#wordsOfLenI = [0 for i in range(1, (max(termsLen) + 1))]\n",
    "#for t in termsLen: \n",
    "#    try:\n",
    "#        wordsOfLenI[t-1] += 1\n",
    "#    except(IndexError):\n",
    "#        print t\n",
    "#print wordsOfLenI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(termsLen, bins=max(termsLen))\n",
    "plt.title(\"Dist. de la long. de las palabras\")\n",
    "plt.xlabel(u\"Tamaño\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.axis(xmin=min(termsLen), xmax=max(termsLen))\n",
    "fig = plt.gcf()\n",
    "plt.show()\n",
    "fig.savefig(\"words_length_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
